{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlM1y3RRwNCj",
        "outputId": "afc4b113-55ff-4994-adfe-3f6b8dfad126",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.42.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.112.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.38.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio requests transformers beautifulsoup4 python-docx torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgyMisms1a78"
      },
      "source": [
        "**Set Up the Environment:** Install the required libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONj5EOsS1kAQ"
      },
      "source": [
        "**Create the Gradio Frontend:** searching for articles, summarizing content, generating citations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PGNKHHx31pF8"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "irs966WMoorK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ypJq7mXRIz7N"
      },
      "outputs": [],
      "source": [
        "\n",
        "def search_related_articles_crossref(query, max_results=3):\n",
        "    \"\"\"Search for related articles using CrossRef API.\"\"\"\n",
        "    try:\n",
        "        url = f\"https://api.crossref.org/works?query={query}&rows={max_results}\"\n",
        "        headers = {\"User-Agent\": \"AI-Powered Research Assistant (your-email@example.com)\"}  # Replace with your email\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            articles = []\n",
        "            data = response.json()\n",
        "            for item in data['message']['items']:\n",
        "                title = item.get('title', ['No Title'])[0]\n",
        "                doi = item.get('DOI', 'No DOI')\n",
        "                link = f\"https://doi.org/{doi}\"\n",
        "                articles.append({\"title\": title, \"link\": link})\n",
        "\n",
        "            print(articles)\n",
        "            if not articles:\n",
        "                print(articles)\n",
        "                return [], \"No articles found for the query.\"\n",
        "            return articles, None\n",
        "        else:\n",
        "            return [], f\"Error fetching articles: {response.status_code} - {response.text}\"\n",
        "    except Exception as e:\n",
        "        return [], f\"Exception during CrossRef API call: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hHWgKauhJX3V"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_from_html(url):\n",
        "    \"\"\"Extract text content from HTML page.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check for request errors\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # This is a simplified example. You may need to adjust the selector based on the site structure.\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text_content = \"\\n\".join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        return text_content\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting text: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TAwypA_GI3I3"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"pszemraj/long-t5-tglobal-base-16384-book-summary\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"pszemraj/long-t5-tglobal-base-16384-book-summary\")\n",
        "\n",
        "def summarize_article(article_text):\n",
        "    \"\"\"Summarize a given article's text.\"\"\"\n",
        "    try:\n",
        "        if not article_text or len(article_text.split()) < 20:\n",
        "            return None, \"Article content is too short to summarize.\"\n",
        "        # Ensure the input text is not too long\n",
        "        inputs = tokenizer(\n",
        "            article_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512,  # Adjust max_length to control input size\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        # Generate the summary\n",
        "        summary_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,  # Limit the length of the output\n",
        "            min_length=100,      # Set a minimum length for the output\n",
        "            # #length_penalty='1.0',  # Adjust length penalty to encourage longer output\n",
        "            # no_repeat_ngram_size=3,    # Avoid repetition of phrases\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode the output to get the summary\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary, None\n",
        "    except Exception as e:\n",
        "        return None, f\"Exception during summarization: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqXw-_yNI5rW",
        "outputId": "e92d779a-5ef4-44dc-c30e-743c201d3c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer_t5 = AutoTokenizer.from_pretrained(\"scieditor/citation-generation-t5\")\n",
        "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"scieditor/citation-generation-t5\")\n",
        "\n",
        "def generate_citation_t5(article_title, citation_style, article_link):\n",
        "    \"\"\"Generate a citation using the T5 or LED model.\"\"\"\n",
        "    try:\n",
        "        # Prepare the input text with explicit and structured formatting\n",
        "        input_text = (f\"'{article_title}'\\n\"\n",
        "                      f\"{article_link}\\n\"\n",
        "                      f\"Include author names, publication date, title, journal name, and DOI if available.\\n\"\n",
        "                      f\"Generate a {citation_style} style citation for the article\")\n",
        "\n",
        "        # Tokenize the input\n",
        "        inputs = tokenizer_t5(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "        # Generate the citation\n",
        "        outputs = model_t5.generate(**inputs, max_new_tokens=70)\n",
        "\n",
        "        # Decode the output to text\n",
        "        citation = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
        "        return citation, None\n",
        "    except Exception as e:\n",
        "        return None, f\"Exception during citation generation: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UFWrWcjGOpaV"
      },
      "outputs": [],
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "def create_thesis_document(title, summary, citations):\n",
        "    \"\"\"Create a Word document formatted like a PhD thesis.\"\"\"\n",
        "\n",
        "    # Initialize Document\n",
        "    doc = Document()\n",
        "\n",
        "    # Title Page\n",
        "    doc.add_paragraph(title, style='Title').alignment = 1  # Center alignment\n",
        "    doc.add_paragraph()  # Add empty line\n",
        "\n",
        "    # Adding title page details\n",
        "    doc.add_paragraph('Thesis', style='Heading 1').alignment = 1\n",
        "    doc.add_paragraph('Author Name', style='Normal').alignment = 1\n",
        "    doc.add_paragraph('University Name', style='Normal').alignment = 1\n",
        "    doc.add_paragraph('Date', style='Normal').alignment = 1\n",
        "\n",
        "    doc.add_page_break()\n",
        "\n",
        "    # Summary Page\n",
        "    doc.add_paragraph('Summary', style='Heading 1').alignment = 0  # Left alignment\n",
        "    doc.add_paragraph(summary, style='Normal')\n",
        "\n",
        "    doc.add_page_break()\n",
        "\n",
        "    # Citation Page\n",
        "    doc.add_paragraph('Citations', style='Heading 1').alignment = 0\n",
        "\n",
        "    for citation in citations:\n",
        "        doc.add_paragraph(citation, style='Normal')\n",
        "\n",
        "    file_path = \"Research_Document.docx\"\n",
        "    doc.save(file_path)\n",
        "    return file_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8z_Ron8mI79k"
      },
      "outputs": [],
      "source": [
        "\n",
        "def research_assistant(research_topic, citation_style):\n",
        "    \"\"\"Main function to search, summarize, and generate citations.\"\"\"\n",
        "    if not research_topic:\n",
        "        return \"Please enter a research topic.\", [\"No summaries generated.\"], [\"No citations generated.\"]\n",
        "\n",
        "    # Character limit check\n",
        "    if len(research_topic) > 150:\n",
        "        return \"Error: Research topic exceeds 150 characters.\", [], []\n",
        "\n",
        "    # Search for related articles using CrossRef\n",
        "    articles, error = search_related_articles_crossref(research_topic)\n",
        "\n",
        "    if error:\n",
        "        return error, [], []\n",
        "\n",
        "    summaries = []\n",
        "    citations = []\n",
        "    article_content = ''\n",
        "\n",
        "    for article in articles:\n",
        "        try:\n",
        "            # Fetching article content might not be feasible; consider using metadata\n",
        "            article_content += f\"{extract_text_from_html(article['link'])}.\\n\"  # Simplified; actual content may require other methods\n",
        "\n",
        "            citation, error = generate_citation_t5(article['title'], citation_style, article['link'])\n",
        "            if error:\n",
        "                citations.append(f\"Error generating citation for '{article['title']}': {error}\")\n",
        "            else:\n",
        "                citations.append(citation)\n",
        "\n",
        "        except Exception as e:\n",
        "            summaries.append(f\"Error processing article '{article['title']}': {str(e)}\")\n",
        "            citations.append(f\"Error generating citation for '{article['title']}': {str(e)}\")\n",
        "\n",
        "    summary, error = summarize_article(article_content)\n",
        "    if error:\n",
        "        summaries.append(f\"Error summarizing article: {error}\")\n",
        "    else:\n",
        "        summaries.append(summary)\n",
        "\n",
        "    file_path = create_thesis_document(research_topic, \"\\n\".join(summaries), citations)\n",
        "    return research_topic, summaries, citations, file_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "6uoYjVVWI-rb",
        "outputId": "9750aeeb-8f9f-4313-8549-3617bd7792ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://592d59cef34f1c5e62.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://592d59cef34f1c5e62.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Create Gradio Interface with download functionality\n",
        "gr_interface = gr.Interface(\n",
        "    fn=research_assistant,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter your research topic or question:\", placeholder=\"Enter your research topic (max 150 characters)\"),\n",
        "        gr.Dropdown(choices=[\"APA\", \"MLA\", \"Chicago\"], label=\"Choose a citation style:\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Research Topic\"),\n",
        "        gr.Textbox(label=\"Summaries of Articles\"),\n",
        "        gr.Textbox(label=\"Generated Citations\"),\n",
        "        gr.DownloadButton(label=\"Download Document\")\n",
        "    ],\n",
        "    title=\"AI-Powered Research Assistant\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "gr_interface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6Kls1xcgXsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8aCkqlVAcuDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5OJ7wsGKeWop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WnpC5yf1ell1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}